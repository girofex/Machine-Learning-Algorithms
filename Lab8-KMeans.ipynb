{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab. 8 - K-Means and the LLoyd algorithm\n",
    "\n",
    "In this lab we consider the problem of **unsupervised learning**, through one of the most famous **clustering** algorithms: K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy.linalg as la\n",
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixGauss(means, sigmas, n):\n",
    "    means = np.array(means)\n",
    "    sigmas = np.array(sigmas)\n",
    "\n",
    "    d = means.shape[1]\n",
    "    num_classes = sigmas.size\n",
    "    data = np.full((n * num_classes, d), np.inf)\n",
    "    labels = np.zeros(n * num_classes)\n",
    "\n",
    "    for idx, sigma in enumerate(sigmas):\n",
    "        data[idx * n:(idx + 1) * n] = np.random.multivariate_normal(\n",
    "            mean=means[idx], cov=np.eye(d) * sigmas[idx] ** 2, size=n)\n",
    "        labels[idx * n:(idx + 1) * n] = idx \n",
    "        \n",
    "    if(num_classes == 2):\n",
    "        labels[labels == 0] = -1\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_distances(X1, X2):\n",
    "    return scipy.spatial.distance.cdist(X1, X2, metric='sqeuclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Implementing the LLoyd Algorithm\n",
    "\n",
    "The Lloyd algorithm is the standard algorithm for implementing k-means. It is based on two steps\n",
    " 1. Assigning each point to a cluster\n",
    " 2. Updating the cluster centers\n",
    "repeated iteratively until the cluster centers (and point assignments) have converged.\n",
    "\n",
    "The function skeleton below takes as input the following arguments\n",
    " - `X` the data matrix\n",
    " - `centers` the initial cluster centers. This could be random, or as we will see in the second part, they could be initialized following a smarter strategy.\n",
    " - `maxiter` the maximum number of iterations of the algorithm.\n",
    " \n",
    "The function returns\n",
    " - the final cluster centers\n",
    " - the assignment labels of each point to their cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyd(X, centers, maxiter):\n",
    "    # X: n x d\n",
    "    # centers : k x d\n",
    "    n, d = X.shape\n",
    "    k = centers.shape[0]\n",
    "    \n",
    "    for i in range(maxiter):\n",
    "        # Compute Squared Euclidean distance (i.e. the squared distance)\n",
    "        # between each cluster centre and each observation\n",
    "        dist = np.abs(centers[i] - X[i]) # ... fill here ...\n",
    "        \n",
    "        # Assign data to clusters: \n",
    "        # for each point, find the closest center in terms of euclidean distance\n",
    "        c_asg = np.argmin(dist)# ... fill here ...\n",
    "\n",
    "        # Update cluster center\n",
    "        for c in range(k):\n",
    "            centers[c] = np.mean(X[c_asg == c])# ... fill here ...\n",
    "        \n",
    "    return c_asg, centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. K-Means: Analysis\n",
    "\n",
    "Here you should create a synthetic dataset using the `mixGauss` function **with four or more classes**.\n",
    "\n",
    "You can experiment with different dataset creation strategies:\n",
    " - Create datasets where the classes are very well separated, then k-means should be able to infer the classes easily\n",
    " - Create datasets where the classes have overlap. In this case the k-means algorithm won't be able to distinguish the points of overlap. What do you think will happen?\n",
    " \n",
    "Then, you should run the k-means algorithm with randomly initialized centers:\n",
    " 1. Create the random centers **within the same range as your data**. You can use the `np.random.uniform` function for this.\n",
    " 2. Run the Lloyd algorithm\n",
    " 3. Plot the results.\n",
    "\n",
    "##### Your Tasks\n",
    "You should repeat this procedure multiple times, and comment on the following:\n",
    " 1. Do you obtain the same clusters every time?\n",
    "     If you obtain clusterings which are always the same, you can try to initialize two cluster centers at a very similar point. This will put the algorithm in a difficult situation!\n",
    "     \n",
    " 2. Why do the colors of a specific cluster seem to change at each iteration?\n",
    " 3. Try using the *wrong* number of clusters `k` (wrong with respect to the number of classes you used to generate the data). What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Build a dataset with at least 4 classes in 2 dimensions, and plot it\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mmixGauss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# ... fill here ...\u001b[39;00m\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(X[:,\u001b[38;5;241m0\u001b[39m], X[:,\u001b[38;5;241m1\u001b[39m], s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m, c\u001b[38;5;241m=\u001b[39mY, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36mmixGauss\u001b[1;34m(means, sigmas, n)\u001b[0m\n\u001b[0;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n \u001b[38;5;241m*\u001b[39m num_classes)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, sigma \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sigmas):\n\u001b[0;32m     11\u001b[0m     data[idx \u001b[38;5;241m*\u001b[39m n:(idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m n] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmultivariate_normal(\n\u001b[1;32m---> 12\u001b[0m         mean\u001b[38;5;241m=\u001b[39m\u001b[43mmeans\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, cov\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39meye(d) \u001b[38;5;241m*\u001b[39m sigmas[idx] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, size\u001b[38;5;241m=\u001b[39mn)\n\u001b[0;32m     13\u001b[0m     labels[idx \u001b[38;5;241m*\u001b[39m n:(idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m n] \u001b[38;5;241m=\u001b[39m idx \n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(num_classes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "# Build a dataset with at least 4 classes in 2 dimensions, and plot it\n",
    "X, Y = mixGauss(means = [[0, 0], [2, 2], [-2, 2], [2, -2]], sigmas = [0.9, 0.75, 1.0, 0.85], n=100)# ... fill here ...\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=Y, alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try with a random initialization of the centers.\n",
    "k = # ... fill here ...\n",
    "centers0 = np.random.uniform(low=np.amin(X, 0), high=np.amax(X, 0), size=(k, X.shape[1]))\n",
    "\n",
    "# Call the lloyd function\n",
    "Iv, centers = # ... fill here ...\n",
    "\n",
    "# Visualize the final clusters and their centroids\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=Iv, marker='*', alpha=0.8)\n",
    "plt.scatter(centers[:,0], centers[:,1], s=70, c='k', alpha=0.8)\n",
    "\n",
    "# NOTE: TRY MULTIPLE TIMES!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. A Better Initialization Strategy: K-Means++\n",
    "\n",
    "Initializing the cluster centers at random, sometimes makes the algorithm converge to a sub-optimal local minimum.\n",
    "\n",
    "The k-means++ algorithm is an **initialization strategy** to generate the initial centers, which can then be passed to the lloyd algorithm.\n",
    "\n",
    "K-means++ works by selecting the cluster centers as the points within the dataset which have the **maximum distance** between each other. To do this it uses a greedy strategy implemented in the `kmeanspp` function below.\n",
    "\n",
    "Take a look at the K-Means++ function and then use it as initialization for the K-means algorithm.\n",
    "\n",
    "##### Your task:\n",
    "You should take a dataset on which K-Means was struggling, and apply K-Means++ for the center initialization, trying to show that the new initialization strategy improves the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeanspp(X, k):\n",
    "    n, d = X.shape\n",
    "\n",
    "    IdxC = np.random.permutation(n)\n",
    "    centers = np.zeros((k, d))\n",
    "\n",
    "    # Select a random point in the dataset as the starting point\n",
    "    centers[0, :] = X[IdxC[0], :]\n",
    "\n",
    "    for i in range(1, k):\n",
    "        D = all_distances(centers[:i, :], X)\n",
    "        Ds = np.min(D, axis=0)  # This is the distance to the closest existing center\n",
    "\n",
    "        # Probability of choosing new points as centers is weighted as the \n",
    "        # squared distance to the closest existing center.\n",
    "        D2 = Ds ** 2\n",
    "        P = np.divide(D2, np.sum(D2))\n",
    "        \n",
    "        # Simply pick the point with the highest probability\n",
    "        newcpos = np.argmax(P)\n",
    "        centers[i,:] = X[newcpos, :]\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try and use K-Means++ for initialization\n",
    "k = # ... fill here ...\n",
    "centers0 = # ... fill here ...\n",
    "\n",
    "# Call the Lloyd function\n",
    "Iv, centers = # ... fill here ...\n",
    "\n",
    "# Visualize the final clusters and their centroids\n",
    "plt.scatter(X[:,0], X[:,1], s=70, c=Iv, alpha=0.8)\n",
    "plt.scatter(centers[:,0], centers[:,1], s=70, c='k', alpha=0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
